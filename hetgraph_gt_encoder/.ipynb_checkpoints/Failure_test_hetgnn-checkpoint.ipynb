{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8aeb17-7759-441e-991c-8bec262d4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['NEO4J_BOLT_URL']='bolt://localhost:7687'\n",
    "os.environ['NEO_PASS']='rl123456'\n",
    "os.environ['NEO_USER']='neo4j'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de96fc-2da9-4a95-9b6c-ee866e04c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_helpers.data_preparation import StateLoader\n",
    "from models.HeCo import HeCo\n",
    "from heco_params import heco_params\n",
    "\n",
    "count_mps=2\n",
    "args = heco_params()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_loader = StateLoader(nr_mps=2, mps=None)\n",
    "(batch_pos1, batch_pos2, batch_neg1), all_state_keys, all_aff_keys, all_obj_keys, action_keys, (fstate_p1, fstate_p2, fstate_n1) = st_loader.get_subgraph_episode_data(batch_size=1)\n",
    "feats = batch_pos1[0][0]\n",
    "nei_index = batch_pos1[0][1]\n",
    "mps = st_loader.generate_mps_episode(nei_index,  fstate_p1)\n",
    "mps_dims = [mp.shape[1] for mp in mps]\n",
    "feats_dim_list = [i.shape[1] for i in batch_pos1[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd806d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_episode(st_loader, batch, full_state):\n",
    "    feats = batch[0][0]\n",
    "    nei_index = batch[0][1]\n",
    "    mps = st_loader.generate_mps_episode(nei_index, full_state)\n",
    "    return feats, nei_index, mps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1afba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_task_incp(model, task, succ):\n",
    "    alpha=0.5\n",
    "    loss_type=None\n",
    "    push_succ_1, all_state_keys, all_aff_keys, all_obj_keys, action_keys, fstate_push_succ_p1, anchor_file_push_succ_1 = st_loader.get_subgraph_episode_data_by_task_incp(task_1=task, succ_fail=succ)\n",
    "    feats_push_succ_1, nei_index_push_succ_1, mps_push_succ_1 = get_data_for_episode(st_loader, push_succ_1, fstate_push_succ_p1)\n",
    "    z_sc_push_succ_1, z_mp_push_succ_1, intra_push_succ_1, inter_push_succ_1 = model(feats_push_succ_1, mps_push_succ_1,nei_index_push_succ_1, alpha, loss_type, testing=True)\n",
    "    return z_sc_push_succ_1, anchor_file_push_succ_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding_task(model, task, succ):\n",
    "    alpha=0.5\n",
    "    loss_type=None\n",
    "    push_succ_1, all_state_keys, all_aff_keys, all_obj_keys, action_keys, fstate_push_succ_p1, anchor_file_push_succ_1 = st_loader.get_subgraph_episode_data_by_task(task_1=task, succ_fail=succ)\n",
    "    feats_push_succ_1, nei_index_push_succ_1, mps_push_succ_1 = get_data_for_episode(st_loader, push_succ_1, fstate_push_succ_p1)\n",
    "    z_sc_push_succ_1, z_mp_push_succ_1, intra_push_succ_1, inter_push_succ_1 = model(feats_push_succ_1, mps_push_succ_1,nei_index_push_succ_1, alpha, loss_type, testing=True)\n",
    "    return z_sc_push_succ_1, anchor_file_push_succ_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs= ['checkpoints_12_cum20', 'checkpoints_15_cum20','checkpoints_hybrid']\n",
    "\n",
    "checkpoints = []\n",
    "for check_dir in dirs:\n",
    "    PATH_TO_CHECKPOINTS = os.path.join(str(os.getcwd()),check_dir)\n",
    "    files = os.listdir(PATH_TO_CHECKPOINTS)\n",
    "\n",
    "    if check_dir == 'checkpoints_hybrid':\n",
    "        file = files[len(files)-1]\n",
    "        print(file)\n",
    "    else:\n",
    "        print(files[21])\n",
    "        file = files[21]\n",
    "    \n",
    "    checkpoint_path = os.path.join(PATH_TO_CHECKPOINTS, file)\n",
    "    checkpoints.append(checkpoint_path)\n",
    "    \n",
    "print(checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_encodings_incp(checkpoint):\n",
    "    tasks = [\"CreateLevelPush-v0\",\n",
    "                       \"CreateLevelBuckets-v0\",\n",
    "                       \"CreateLevelBasket-v0\",\"CreateLevelBelt-v0\",\n",
    "                       \"CreateLevelObstacle-v0\"]\n",
    "    succ_fail = ['true', 'false']\n",
    "    model = HeCo(args.hidden_dim, feats_dim_list, args.feat_drop, args.attn_drop,\n",
    "                 count_mps, args.sample_rate, args.nei_num, args.tau, args.lam, mps_dims).to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "    model.eval()\n",
    "    z_encodings = []\n",
    "    labels = []\n",
    "    # Loop through tasks and outcomes to fetch episodes\n",
    "    for task in tasks:\n",
    "        for outcome in succ_fail:\n",
    "            for _ in range(num_episodes_per_type):\n",
    "                try:\n",
    "                    z_sc, anchor_file = get_encoding_task_incp(model, task, outcome)\n",
    "                    z_encodings.append(z_sc)\n",
    "                    labels.append((task, outcome))\n",
    "                except:\n",
    "                    print(task)\n",
    "                    continue\n",
    "\n",
    "    # Convert list of tensors to a tensor for efficient operations\n",
    "    z_encodings = torch.stack(z_encodings)\n",
    "    return z_encodings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7da613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_encodings(checkpoint):\n",
    "    tasks = [\"CreateLevelPush-v0\",\n",
    "                       \"CreateLevelBuckets-v0\",\n",
    "                       \"CreateLevelBasket-v0\",\"CreateLevelBelt-v0\",\n",
    "                       \"CreateLevelObstacle-v0\"]\n",
    "    succ_fail = ['true', 'false']\n",
    "    model = HeCo(args.hidden_dim, feats_dim_list, args.feat_drop, args.attn_drop,\n",
    "                 count_mps, args.sample_rate, args.nei_num, args.tau, args.lam, mps_dims).to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "    model.eval()\n",
    "    z_encodings = []\n",
    "    labels = []\n",
    "    # Loop through tasks and outcomes to fetch episodes\n",
    "    for task in tasks:\n",
    "        for outcome in succ_fail:\n",
    "            for _ in range(num_episodes_per_type):\n",
    "                try:\n",
    "                    z_sc, anchor_file = get_encoding_task(model, task, outcome)\n",
    "                    z_encodings.append(z_sc)\n",
    "                    labels.append((task, outcome))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(task)\n",
    "                    continue\n",
    "\n",
    "    # Convert list of tensors to a tensor for efficient operations\n",
    "    z_encodings = torch.stack(z_encodings)\n",
    "    return z_encodings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def compute_clusters_by_outcome(z_encodings, labels, tasks):\n",
    "    # Convert tensor to numpy for sklearn compatibility\n",
    "    z_encodings_np = z_encodings.cpu().detach().numpy()\n",
    "    z_encodings_np = z_encodings_np.squeeze(1)  # Ensure the encodings are 2D: [num_samples, feature_dim]\n",
    "\n",
    "    # Prepare dataframes for success and failure episodes\n",
    "    df = pd.DataFrame({\n",
    "        'task': [label[0] for label in labels],  # Original task names\n",
    "        'outcome': [label[1] for label in labels]  # Success/Failure outcome\n",
    "    })\n",
    "    \n",
    "    # Filter the encodings based on success and failure\n",
    "    success_indices = df[df['outcome'] == 'true'].index\n",
    "    failure_indices = df[df['outcome'] == 'false'].index\n",
    "\n",
    "    z_success = z_encodings_np[success_indices]\n",
    "    z_failure = z_encodings_np[failure_indices]\n",
    "\n",
    "    # Define number of clusters as the number of unique tasks\n",
    "    n_clusters = len(tasks)\n",
    "\n",
    "    # Perform K-means clustering on successful episodes\n",
    "    kmeans_success = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels_success = kmeans_success.fit_predict(z_success)\n",
    "    \n",
    "    # Create a DataFrame for successful episodes with their cluster labels\n",
    "    df_clusters_success = pd.DataFrame({\n",
    "        'task': df.loc[success_indices, 'task'].values,\n",
    "        'cluster': cluster_labels_success\n",
    "    })\n",
    "\n",
    "    # Perform K-means clustering on unsuccessful episodes\n",
    "    kmeans_failure = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels_failure = kmeans_failure.fit_predict(z_failure)\n",
    "\n",
    "    # Create a DataFrame for unsuccessful episodes with their cluster labels\n",
    "    df_clusters_failure = pd.DataFrame({\n",
    "        'task': df.loc[failure_indices, 'task'].values,\n",
    "        'cluster': cluster_labels_failure\n",
    "    })\n",
    "\n",
    "    # Visualize the clusters for successful episodes using PCA\n",
    "    pca_success = PCA(n_components=2)\n",
    "    z_success_pca = pca_success.fit_transform(z_success)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        x=z_success_pca[:, 0], y=z_success_pca[:, 1],\n",
    "        hue=df_clusters_success['task'],\n",
    "        palette='tab10', s=100\n",
    "    )\n",
    "    plt.title('K-means Clustering of Successful Episodes (PCA Reduced)')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Task')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize the clusters for unsuccessful episodes using PCA\n",
    "    pca_failure = PCA(n_components=2)\n",
    "    z_failure_pca = pca_failure.fit_transform(z_failure)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        x=z_failure_pca[:, 0], y=z_failure_pca[:, 1],\n",
    "        hue=df_clusters_failure['task'],\n",
    "        palette='tab10', s=100\n",
    "    )\n",
    "    plt.title('K-means Clustering of Unsuccessful Episodes (PCA Reduced)')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Task')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate clustering by task for success\n",
    "    print(\"Cluster counts per task for successful episodes:\")\n",
    "    print(df_clusters_success.groupby(['task', 'cluster']).size())\n",
    "\n",
    "    # Evaluate clustering by task for failure\n",
    "    print(\"\\nCluster counts per task for unsuccessful episodes:\")\n",
    "    print(df_clusters_failure.groupby(['task', 'cluster']).size())\n",
    "\n",
    "    return df_clusters_success, df_clusters_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_encodings_diff_checkpoints_incp = []\n",
    "z_encoding_labels_incp = []\n",
    "num_episodes_per_type = 5\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)\n",
    "    z_enc, labels = compute_encodings_incp(checkpoint)\n",
    "    z_encoding_labels_incp.append(labels)\n",
    "    z_encodings_diff_checkpoints_incp.append(z_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_encodings_diff_checkpoints = []\n",
    "z_encoding_labels = []\n",
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)\n",
    "    z_enc, labels = compute_encodings(checkpoint)\n",
    "    z_encoding_labels.append(labels)\n",
    "    z_encodings_diff_checkpoints.append(z_enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
